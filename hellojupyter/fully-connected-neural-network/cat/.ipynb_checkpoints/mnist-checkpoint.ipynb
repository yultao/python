{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc as misc\n",
    "from scipy import ndimage\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNMnist():\n",
    "    # GRADED FUNCTION: random_mini_batches\n",
    "    \n",
    "    def random_mini_batches(self, X, Y, mini_batch_size = 64, seed = 0):\n",
    "        np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "        m = X.shape[1]                  # number of training examples\n",
    "        mini_batches = []\n",
    "            \n",
    "        # Step 1: Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "    \n",
    "        # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            ### START CODE HERE ### (approx. 2 lines)\n",
    "            mini_batch_X = shuffled_X[:, mini_batch_size*k : (k+1)*mini_batch_size]\n",
    "            mini_batch_Y = shuffled_Y[:, mini_batch_size*k : (k+1)*mini_batch_size]\n",
    "            ### END CODE HERE ###\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        # Handling the end case (last mini-batch < mini_batch_size)\n",
    "        if m % mini_batch_size != 0:\n",
    "            ### START CODE HERE ### (approx. 2 lines)\n",
    "            mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size : m]\n",
    "            mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size : m]\n",
    "            ### END CODE HERE ###\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        return mini_batches\n",
    "    def sigmoid(self, Z):\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "        cache = Z\n",
    "        return A, cache\n",
    "    def relu(self, Z):\n",
    "        \n",
    "        A = np.maximum(0,Z)\n",
    "        \n",
    "        assert(A.shape == Z.shape)\n",
    "        \n",
    "        cache = Z \n",
    "        return A, cache\n",
    "    \n",
    "    def relu_backward(self, dA, cache):\n",
    "        \n",
    "        Z = cache\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "        \n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "        \n",
    "        assert (dZ.shape == Z.shape)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def sigmoid_backward(self, dA, cache):\n",
    "        \n",
    "        Z = cache\n",
    "        \n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        dZ = dA * s * (1-s)\n",
    "        \n",
    "        assert (dZ.shape == Z.shape)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    # GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "    def initialize_parameters_deep(self, layer_dims):\n",
    "\n",
    "        \n",
    "        np.random.seed(3)\n",
    "        parameters = {}\n",
    "        L = len(layer_dims)            # number of layers in the network\n",
    "    \n",
    "        for l in range(1, L):\n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) *0.01 #GT: remove 0.01\n",
    "            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "            assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    \n",
    "            \n",
    "        return parameters\n",
    "    \n",
    "    # GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "    def initialize_parameters_deep_L_layer_model(self, layer_dims):\n",
    "      \n",
    "        np.random.seed(1) # GT: change from 3 to 1\n",
    "        parameters = {}\n",
    "        L = len(layer_dims)            # number of layers in the network\n",
    "    \n",
    "        for l in range(1, L):\n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01 GT: remove 0.01\n",
    "            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "            assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    \n",
    "            \n",
    "        return parameters\n",
    "    \n",
    "    # GRADED FUNCTION: initialize_parameters_he\n",
    "    \n",
    "    def initialize_parameters_he(self, layers_dims):\n",
    "       \n",
    "        np.random.seed(1)\n",
    "        parameters = {}\n",
    "        L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "         \n",
    "        for l in range(1, L + 1):\n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1]) * np.sqrt(2./layers_dims[l-1])\n",
    "            parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "        return parameters\n",
    "    # GRADED FUNCTION: linear_forward\n",
    "    \n",
    "    def linear_forward(self, A_prev, W, b): #GT: change A to A_prev to avoid confusion\n",
    "        \n",
    "        \n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        Z = np.dot(W,A_prev)+b\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        cache = (A_prev, W, b)\n",
    "        \n",
    "        return Z, cache\n",
    "    # GRADED FUNCTION: linear_activation_forward\n",
    "    \n",
    "    def linear_activation_forward(self, A_prev, W, b, activation):\n",
    "        \n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.sigmoid(Z)\n",
    "            ### END CODE HERE ###\n",
    "        \n",
    "        elif activation == \"relu\":\n",
    "            # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.relu(Z)\n",
    "            ### END CODE HERE ###\n",
    "        \n",
    "        assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        cache = (linear_cache, activation_cache)#(A_prev, W, b, Z)\n",
    "    \n",
    "        return A, cache\n",
    "    \n",
    "    # GRADED FUNCTION: L_model_forward\n",
    "    \n",
    "    def L_model_forward(self, X, parameters):\n",
    "        \n",
    "    \n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(parameters) // 2                  # number of layers in the neural network\n",
    "        \n",
    "        # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "        for l in range(1, L):\n",
    "            A_prev = A \n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            A, cache = self.linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "            caches.append(cache)\n",
    "            ### END CODE HERE ###\n",
    "        \n",
    "        # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        AL, cache = self.linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        if(len(np.where(AL == 0)[0])!=0):\n",
    "#             print(\"Has 0 compute_cost\")\n",
    "            zero = np.where(AL == 0)\n",
    "            AL[zero]=0.00000000001\n",
    "        if(len(np.where(AL == 1)[0])!=0):\n",
    "#             print(\"Has 1 compute_cost\")\n",
    "            one = np.where(AL == 1)\n",
    "#             print(AL[np.where(AL == 1)[0],np.where(AL == 1)[1]])\n",
    "            AL[one]=1-0.00000000001\n",
    "#             print(AL[np.where(AL == 1)[0],np.where(AL == 1)[1]])\n",
    "        \n",
    "        \n",
    "#         assert(AL.shape == (1,X.shape[1]))\n",
    "                \n",
    "        return AL, caches\n",
    "    \n",
    "    def compute_cost(self, AL, Y, parameters, lambd):\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Compute loss from aL and y.\n",
    "        ### START CODE HERE ### (≈ 1 lines of code)\n",
    "#         AL += 0.000000001\n",
    "\n",
    "\n",
    "\n",
    "#         print(AL)\n",
    "#         print(parameters[\"W1\"][:,0:10])\n",
    "        cost = -(1/m) * np.sum(     Y * np.nan_to_num(np.log(AL)) +   (1-Y) * np.nan_to_num(np.log(1-AL))  ) \n",
    "        \n",
    "        cost= np.nan_to_num(cost)\n",
    "        #calc cost reg\n",
    "        L2_regularization_cost = 0\n",
    "        L = len(parameters)//2\n",
    "        for l in range(1, L + 1):\n",
    "            W = parameters['W' + str(l)]\n",
    "            L2_regularization_cost += np.sum(np.square(W))\n",
    "#         W1 = parameters[\"W1\"]\n",
    "#         W2 = parameters[\"W2\"]\n",
    "#         W3 = parameters[\"W3\"]\n",
    "#         L2_regularization_cost = (1/m)*(lambd/2)*(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "        L2_regularization_cost = (1/m)*(lambd/2) * L2_regularization_cost\n",
    "        \n",
    "        \n",
    "        cost = cost + L2_regularization_cost\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "        assert(cost.shape == ())\n",
    "        \n",
    "        return cost \n",
    "    \n",
    "    def linear_backward(self, dZ, cache, lambd):\n",
    "        \n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "    \n",
    "        ### START CODE HERE ### (≈ 3 lines of code)\n",
    "        dW = (1/m)*np.dot(dZ, A_prev.T) + (lambd/m)*W\n",
    "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (dW.shape == W.shape)\n",
    "        assert (db.shape == b.shape)\n",
    "        \n",
    "        return dA_prev, dW, db    \n",
    "    \n",
    "    # GRADED FUNCTION: linear_activation_backward\n",
    "    \n",
    "    def linear_activation_backward(self, dA, cache, activation, lambd):\n",
    "        \n",
    "        linear_cache, activation_cache = cache\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            dZ = self.relu_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.linear_backward(dZ, linear_cache, lambd)\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "        elif activation == \"sigmoid\":\n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            dZ = self.sigmoid_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.linear_backward(dZ, linear_cache, lambd)\n",
    "            ### END CODE HERE ###\n",
    "        \n",
    "        return dA_prev, dW, db    \n",
    "    \n",
    "    # GRADED FUNCTION: L_model_backward\n",
    "    \n",
    "    def L_model_backward(self, AL, Y, caches, lambd):\n",
    "        \n",
    "        grads = {}\n",
    "        L = len(caches) # the number of layers\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "        \n",
    "        # Initializing the backpropagation\n",
    "        ### START CODE HERE ### (1 line of code)\n",
    "        if(len(np.where(AL == 0)[0])!=0):\n",
    "            print(\"Has 0 L_model_backward\")\n",
    "        if(len(np.where(AL == 1)[0])!=0):\n",
    "            print(\"Has 1 L_model_backward\")\n",
    "        dAL = - (np.nan_to_num(np.divide(Y, AL)) - np.nan_to_num(np.divide(1 - Y, 1 - AL)))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        current_cache = caches[L-1]\n",
    "        grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = self.linear_activation_backward(dAL, current_cache, \"sigmoid\", lambd)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        for l in reversed(range(L-1)):\n",
    "            # lth layer: (RELU -> LINEAR) gradients.\n",
    "            # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "            ### START CODE HERE ### (approx. 5 lines)\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = self.linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\", lambd)\n",
    "            grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "            ### END CODE HERE ###\n",
    "    \n",
    "        return grads  \n",
    "\n",
    "    def update_parameters(self, parameters, grads, learning_rate):\n",
    "        \n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "        # Update rule for each parameter. Use a for loop.\n",
    "        ### START CODE HERE ### (≈ 3 lines of code)\n",
    "        for l in range(1, L+1):\n",
    "            parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "            parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "        ### END CODE HERE ###\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X, y, parameters):\n",
    "\n",
    "        \n",
    "        m = X.shape[1]\n",
    "        n = len(parameters) // 2 # number of layers in the neural network\n",
    "        p = np.zeros((y.shape[0],m))\n",
    "        \n",
    "        # Forward propagation\n",
    "        probas, caches = self.L_model_forward(X, parameters)\n",
    "    \n",
    "        \n",
    "        # convert probas and y to 0-9 predictions\n",
    "        p = np.argmax(probas, axis=0).reshape(1,m)\n",
    "        y_orig = np.argmax(y, axis=0).reshape(1,m)\n",
    "\n",
    "        print(\"Accuracy: \"  + str(np.sum((p == y_orig)/m)))\n",
    "            \n",
    "        return p\n",
    "    \n",
    "    \n",
    "    # GRADED FUNCTION: L_layer_model\n",
    "    def L_layer_model(self, X, Y, layers_dims, learning_rate = 0.0075, \n",
    "                                          print_cost=False, lambd=0,\n",
    "                                          num_epochs = 1000, mini_batch_size = 64,\n",
    "                                          print_cost_interval=1\n",
    "                                          ):\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        costs = []                         # keep track of cost\n",
    "        \n",
    "        # Parameters initialization.\n",
    "        ### START CODE HERE ###\n",
    "        parameters = self.initialize_parameters_deep_L_layer_model(layers_dims)\n",
    "        ### END CODE HERE ###\n",
    "        seed = 10\n",
    "        # Loop (gradient descent)\n",
    "        for i in range(num_epochs):\n",
    "            print(\"epcho: \"+str(i))\n",
    "            seed = seed + 1\n",
    "            minibatches = self.random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "            num_minibatches = len(minibatches)\n",
    "            num_mb = 0\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "#                 num_mb += 1\n",
    "#                 print(str(num_mb)+\"/\"+str(num_minibatches))\n",
    "                \n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "                ### START CODE HERE ### (≈ 1 line of code)\n",
    "                AL, caches = self.L_model_forward(minibatch_X, parameters)\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                # Compute cost.\n",
    "                ### START CODE HERE ### (≈ 1 line of code)\n",
    "                cost = self.compute_cost(AL, minibatch_Y, parameters, lambd)\n",
    "                ### END CODE HERE ###\n",
    "            \n",
    "                # Backward propagation.\n",
    "                ### START CODE HERE ### (≈ 1 line of code)\n",
    "                grads = self.L_model_backward(AL, minibatch_Y, caches, lambd)\n",
    "                ### END CODE HERE ###\n",
    "         \n",
    "                # Update parameters.\n",
    "                ### START CODE HERE ### (≈ 1 line of code)\n",
    "                parameters = self.update_parameters(parameters, grads, learning_rate)\n",
    "                ### END CODE HERE ###\n",
    "                        \n",
    "            # Print the cost every 100 training example\n",
    "            if print_cost and i % print_cost_interval == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            if print_cost and i % print_cost_interval == 0:\n",
    "                costs.append(cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('epochs (per 100)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test cases\n",
    "\"\"\"\n",
    "class NNMnistTestCase():\n",
    "    def load_cat_data(self):\n",
    "        train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "        train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "        train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    \n",
    "        test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "        test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "        test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "    \n",
    "        classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "        \n",
    "        train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "        test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "        \n",
    "        return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes    \n",
    "    def load_mnist_data(self):\n",
    "        mnist = fetch_mldata('MNIST original')\n",
    "#         print(mnist.data.shape)\n",
    "#         print(mnist.target.shape)\n",
    "        return mnist;\n",
    "    def linear_forward_test_case(self):\n",
    "        np.random.seed(1)\n",
    "        \"\"\"\n",
    "        X = np.array([[-1.02387576, 1.12397796],\n",
    "     [-1.62328545, 0.64667545],\n",
    "     [-1.74314104, -0.59664964]])\n",
    "        W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])\n",
    "        b = np.array([[1]])\n",
    "        \"\"\"\n",
    "        A = np.random.randn(3,2)\n",
    "        W = np.random.randn(1,3)\n",
    "        b = np.random.randn(1,1)\n",
    "        \n",
    "        return A, W, b\n",
    "    \n",
    "    def linear_activation_forward_test_case(self):\n",
    "        \"\"\"\n",
    "        X = np.array([[-1.02387576, 1.12397796],\n",
    "     [-1.62328545, 0.64667545],\n",
    "     [-1.74314104, -0.59664964]])\n",
    "        W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])\n",
    "        b = 5\n",
    "        \"\"\"\n",
    "        np.random.seed(2)\n",
    "        A_prev = np.random.randn(3,2)\n",
    "        W = np.random.randn(1,3)\n",
    "        b = np.random.randn(1,1)\n",
    "        return A_prev, W, b\n",
    "    \n",
    "    def L_model_forward_test_case(self):\n",
    "        \"\"\"\n",
    "        X = np.array([[-1.02387576, 1.12397796],\n",
    "     [-1.62328545, 0.64667545],\n",
    "     [-1.74314104, -0.59664964]])\n",
    "        parameters = {'W1': np.array([[ 1.62434536, -0.61175641, -0.52817175],\n",
    "            [-1.07296862,  0.86540763, -2.3015387 ]]),\n",
    "     'W2': np.array([[ 1.74481176, -0.7612069 ]]),\n",
    "     'b1': np.array([[ 0.],\n",
    "            [ 0.]]),\n",
    "     'b2': np.array([[ 0.]])}\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        X = np.random.randn(4,2)\n",
    "        W1 = np.random.randn(3,4)\n",
    "        b1 = np.random.randn(3,1)\n",
    "        W2 = np.random.randn(1,3)\n",
    "        b2 = np.random.randn(1,1)\n",
    "        parameters = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2}\n",
    "        \n",
    "        return X, parameters\n",
    "    \n",
    "    def compute_cost_test_case(self):\n",
    "        Y = np.asarray([[1, 1, 1]])\n",
    "        aL = np.array([[.8,.9,0.4]])\n",
    "        \n",
    "        return Y, aL\n",
    "    \n",
    "    def linear_backward_test_case(self):\n",
    "        \"\"\"\n",
    "        z, linear_cache = (np.array([[-0.8019545 ,  3.85763489]]), (np.array([[-1.02387576,  1.12397796],\n",
    "           [-1.62328545,  0.64667545],\n",
    "           [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), np.array([[1]]))\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        dZ = np.random.randn(1,2)\n",
    "        A = np.random.randn(3,2)\n",
    "        W = np.random.randn(1,3)\n",
    "        b = np.random.randn(1,1)\n",
    "        linear_cache = (A, W, b)\n",
    "        return dZ, linear_cache\n",
    "    \n",
    "    def linear_activation_backward_test_case(self):\n",
    "        \"\"\"\n",
    "        aL, linear_activation_cache = (np.array([[ 3.1980455 ,  7.85763489]]), ((np.array([[-1.02387576,  1.12397796], [-1.62328545,  0.64667545], [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5), np.array([[ 3.1980455 ,  7.85763489]])))\n",
    "        \"\"\"\n",
    "        np.random.seed(2)\n",
    "        dA = np.random.randn(1,2)\n",
    "        A = np.random.randn(3,2)\n",
    "        W = np.random.randn(1,3)\n",
    "        b = np.random.randn(1,1)\n",
    "        Z = np.random.randn(1,2)\n",
    "        linear_cache = (A, W, b)\n",
    "        activation_cache = Z\n",
    "        linear_activation_cache = (linear_cache, activation_cache)\n",
    "        \n",
    "        return dA, linear_activation_cache\n",
    "    \n",
    "    def L_model_backward_test_case(self):\n",
    "        \"\"\"\n",
    "        X = np.random.rand(3,2)\n",
    "        Y = np.array([[1, 1]])\n",
    "        parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747]]), 'b1': np.array([[ 0.]])}\n",
    "    \n",
    "        aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],\n",
    "               [ 0.02738759,  0.67046751],\n",
    "               [ 0.4173048 ,  0.55868983]]),\n",
    "        np.array([[ 1.78862847,  0.43650985,  0.09649747]]),\n",
    "        np.array([[ 0.]])),\n",
    "       np.array([[ 0.41791293,  1.91720367]]))])\n",
    "       \"\"\"\n",
    "        np.random.seed(3)\n",
    "        AL = np.random.randn(1, 2)\n",
    "        Y = np.array([[1, 0]])\n",
    "    \n",
    "        A1 = np.random.randn(4,2)\n",
    "        W1 = np.random.randn(3,4)\n",
    "        b1 = np.random.randn(3,1)\n",
    "        Z1 = np.random.randn(3,2)\n",
    "        linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
    "    \n",
    "        A2 = np.random.randn(3,2)\n",
    "        W2 = np.random.randn(1,3)\n",
    "        b2 = np.random.randn(1,1)\n",
    "        Z2 = np.random.randn(1,2)\n",
    "        linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
    "    \n",
    "        caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "    \n",
    "        return AL, Y, caches\n",
    "    \n",
    "    def update_parameters_test_case(self):\n",
    "        \n",
    "        np.random.seed(2)\n",
    "        W1 = np.random.randn(3,4)\n",
    "        b1 = np.random.randn(3,1)\n",
    "        W2 = np.random.randn(1,3)\n",
    "        b2 = np.random.randn(1,1)\n",
    "        parameters = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2}\n",
    "        np.random.seed(3)\n",
    "        dW1 = np.random.randn(3,4)\n",
    "        db1 = np.random.randn(3,1)\n",
    "        dW2 = np.random.randn(1,3)\n",
    "        db2 = np.random.randn(1,1)\n",
    "        grads = {\"dW1\": dW1,\n",
    "                 \"db1\": db1,\n",
    "                 \"dW2\": dW2,\n",
    "                 \"db2\": db2}\n",
    "        \n",
    "        return parameters, grads\n",
    "    \n",
    "    \n",
    "    def L_model_forward_test_case_2hidden(self):\n",
    "        np.random.seed(6)\n",
    "        X = np.random.randn(5,4)\n",
    "        W1 = np.random.randn(4,5)\n",
    "        b1 = np.random.randn(4,1)\n",
    "        W2 = np.random.randn(3,4)\n",
    "        b2 = np.random.randn(3,1)\n",
    "        W3 = np.random.randn(1,3)\n",
    "        b3 = np.random.randn(1,1)\n",
    "      \n",
    "        parameters = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2,\n",
    "                      \"W3\": W3,\n",
    "                      \"b3\": b3}\n",
    "        \n",
    "        return X, parameters\n",
    "    \n",
    "    def print_grads(self, grads):\n",
    "        print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "        print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "        print (\"dA1 = \"+ str(grads[\"dA2\"])) # this is done on purpose to be consistent with lecture where we normally start with A0\n",
    "                                        # in this implementation we started with A1, hence we bump it up by 1. \n",
    "    def random_mini_batches_test_case(self):\n",
    "        np.random.seed(1)\n",
    "        mini_batch_size = 64\n",
    "        X = np.random.randn(12288, 148)\n",
    "        Y = np.random.randn(1, 148) < 0.5\n",
    "        return X, Y, mini_batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test\n",
    "\"\"\"\n",
    "\n",
    "def testComputeCost():\n",
    "    dnn = NNMnist()\n",
    "    dnntc= NNMnistTestCase()    \n",
    "    print(\"\\n===================compute_cost\\n\")\n",
    "    Y, AL = dnntc.compute_cost_test_case()\n",
    "    print(\"cost = expected: 0.414931599615, actual: \" + str(dnn.compute_cost(AL, Y, [], 0)))\n",
    "#     cost = \n",
    "def testMinibatch():\n",
    "    dnn = NNMnist()\n",
    "    dnntc= NNMnistTestCase()    \n",
    "    X_assess, Y_assess, mini_batch_size = dnntc.random_mini_batches_test_case()\n",
    "    mini_batches = dnn.random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
    "    \n",
    "    print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "    print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "    print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "    print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "    print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "    print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "    print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n",
    "    \n",
    "    \n",
    "#     shape of the 1st mini_batch_X: (12288, 64)\n",
    "#     shape of the 2nd mini_batch_X: (12288, 64)\n",
    "#     shape of the 3rd mini_batch_X: (12288, 20)\n",
    "#     shape of the 1st mini_batch_Y: (1, 64)\n",
    "#     shape of the 2nd mini_batch_Y: (1, 64)\n",
    "#     shape of the 3rd mini_batch_Y: (1, 20)\n",
    "#     mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]\n",
    "def testOptimalMinibatchMnist():\n",
    "    dnn = NNMnist()\n",
    "    dnntc= NNMnistTestCase()\n",
    "    \n",
    "    mnist = dnntc.load_mnist_data()\n",
    "    m,n = mnist.data.shape\n",
    "    mnist.target = mnist.target.reshape(m,1)\n",
    "    num_labels = 10\n",
    "    e=np.eye(num_labels)\n",
    "\n",
    "    #shuffle\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = mnist.data[permutation,:]\n",
    "    shuffled_Y = mnist.target[permutation,:]\n",
    "    \n",
    "    shuffled_X = shuffled_X / 255\n",
    "    \n",
    "    \n",
    "    train_x = shuffled_X[0:50000,:].T  #784x50000\n",
    "    train_y =  shuffled_Y[0:50000,:].T #1x50000\n",
    "    train_y = e[train_y[0,:].astype(int),:].T#10x50000\n",
    "    \n",
    "    dev_x = shuffled_X[50000:60000,:].T\n",
    "    dev_y=  shuffled_Y[50000:60000,:].T\n",
    "    dev_y = e[dev_y[0,:].astype(int),:].T#10x50000\n",
    "   \n",
    "    test_x = shuffled_X[60000:70000,:].T\n",
    "    test_y=  shuffled_Y[60000:70000,:].T\n",
    "    test_y = e[test_y[0,:].astype(int),:].T#10x50000\n",
    "    \n",
    "    \n",
    "    print(\"train set shape\")\n",
    "    print(train_x.shape)\n",
    "    print(train_y.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    layers_dims =  [n, 30, 30, num_labels]#  5-layer model\n",
    "    print(\"\\n===================L_layer_model \\n\")\n",
    "    parameters = dnn.L_layer_model(\n",
    "        train_x, train_y, layers_dims,  print_cost = True, \n",
    "        lambd = 0, learning_rate = 0.1, \n",
    "        mini_batch_size=10, num_epochs=30,\n",
    "        print_cost_interval=1)\n",
    "     \n",
    "    print(\"\\n===================predict train\\n\")\n",
    "    pred_train = dnn.predict(train_x, train_y, parameters)\n",
    "     \n",
    "    print(\"\\n===================predict dev\\n\")\n",
    "    pred_dev = dnn.predict(dev_x, dev_y, parameters)\n",
    "if __name__ == '__main__':\n",
    "    testOptimalMinibatchMnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape\n",
      "(784, 50000)\n",
      "(10, 50000)\n",
      "\n",
      "===================L_layer_model \n",
      "\n",
      "epcho: 0\n",
      "Cost after iteration 0: 0.801381\n",
      "epcho: 1\n",
      "Cost after iteration 1: 0.389865\n",
      "epcho: 2\n",
      "Cost after iteration 2: 0.049212\n",
      "epcho: 3\n",
      "Cost after iteration 3: 0.404393\n",
      "epcho: 4\n",
      "Cost after iteration 4: 0.050365\n",
      "epcho: 5\n",
      "Cost after iteration 5: 0.018749\n",
      "epcho: 6\n",
      "Cost after iteration 6: 0.028769\n",
      "epcho: 7\n",
      "Cost after iteration 7: 0.007905\n",
      "epcho: 8\n",
      "Cost after iteration 8: 0.020183\n",
      "epcho: 9\n",
      "Cost after iteration 9: 0.346665\n",
      "epcho: 10\n",
      "Cost after iteration 10: 0.021347\n",
      "epcho: 11\n",
      "Cost after iteration 11: 0.065723\n",
      "epcho: 12\n",
      "Cost after iteration 12: 0.001380\n",
      "epcho: 13\n",
      "Cost after iteration 13: 0.018317\n",
      "epcho: 14\n",
      "Cost after iteration 14: 0.007031\n",
      "epcho: 15\n",
      "Cost after iteration 15: 0.515758\n",
      "epcho: 16\n",
      "Cost after iteration 16: 0.176750\n",
      "epcho: 17\n",
      "Cost after iteration 17: 2.154068\n",
      "epcho: 18\n",
      "Cost after iteration 18: 0.000108\n",
      "epcho: 19\n",
      "Cost after iteration 19: 0.107479\n",
      "epcho: 20\n",
      "Cost after iteration 20: 0.011705\n",
      "epcho: 21\n",
      "Cost after iteration 21: 0.208524\n",
      "epcho: 22\n",
      "Cost after iteration 22: 0.601986\n",
      "epcho: 23\n",
      "Cost after iteration 23: 0.486141\n",
      "epcho: 24\n",
      "Cost after iteration 24: 0.002627\n",
      "epcho: 25\n",
      "Cost after iteration 25: 0.358140\n",
      "epcho: 26\n",
      "Cost after iteration 26: 0.047211\n",
      "epcho: 27\n",
      "Cost after iteration 27: 0.001589\n",
      "epcho: 28\n",
      "Cost after iteration 28: 0.562590\n",
      "epcho: 29\n",
      "Cost after iteration 29: 0.001637\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKCillAr5/6ktwKi+toEcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19ba62b17f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================predict train\n",
      "\n",
      "Accuracy: 0.98648\n",
      "\n",
      "===================predict dev\n",
      "\n",
      "Accuracy: 0.961\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
